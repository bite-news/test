import os
import json
import requests
import logging
import time
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# âœ… í™˜ê²½ ë³€ìˆ˜ì—ì„œ API ì—”ë“œí¬ì¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
FASTAPI_SERVER_URL = os.getenv("FASTAPI_SERVER_URL", "http://3.34.177.106:8000/ai/video")

# âœ… í—¤ë” ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
header = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                   "(KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36"),
}

# âœ… í˜„ì¬ ì‹œê°„ (UTC+9 ê¸°ì¤€)
CURRENT_TIME = datetime.now()

# âœ… í¬ë¡¤ë§í•  ë„¤ì´ë²„ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬
categories = {
    "ì •ì¹˜": "https://news.naver.com/section/100",
    "ê²½ì œ": "https://news.naver.com/section/101",
    "ì‚¬íšŒ": "https://news.naver.com/section/102",
}

# ğŸ“° ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§ í•¨ìˆ˜
def scrape_article(article_url):
    try:
        time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì • (ë„¤ì´ë²„ ì„œë²„ ì°¨ë‹¨ ë°©ì§€)
        response = requests.get(article_url, headers=header)
        soup = BeautifulSoup(response.text, "html.parser")

        # âœ… ê¸°ì‚¬ ì œëª© ì°¾ê¸°
        title_tag = soup.find("h2", {"id": "title_area"})
        title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

        # âœ… ê¸°ì‚¬ ë³¸ë¬¸ ì°¾ê¸°
        content_tag = soup.find("article", {"id": "dic_area"})
        content = content_tag.text.strip().replace('\\"', '"').replace("\n", " ") if content_tag else "ë³¸ë¬¸ ì—†ìŒ"

        # âœ… ê¸°ì‚¬ ìƒì„± ì‹œê°„ ì°¾ê¸°
        timestamp_tag = soup.find("span", {"class": "media_end_head_info_datestamp_time _ARTICLE_DATE_TIME"}) 
        if timestamp_tag and "data-date-time" in timestamp_tag.attrs:
            article_time = datetime.strptime(timestamp_tag["data-date-time"], "%Y-%m-%d %H:%M:%S")
        else:
            article_time = None

        return {
            "title": title,
            "content": content,
            "timestamp": article_time,
            "link": article_url
        }
    except Exception as e:
        logging.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {article_url}, ì˜¤ë¥˜: {e}")
        return None

# ğŸš€ FastAPI ì„œë²„ë¡œ ë°ì´í„° ì „ì†¡ í•¨ìˆ˜
def post_article_to_server(article_data):
    headers = {"Content-Type": "application/json"}
    try:
        logging.info(f"ğŸ“¤ BiteNewsì— ê¸°ì‚¬ ì „ì†¡ ì¤‘: {article_data['title']}")
        response = requests.post(FASTAPI_SERVER_URL, json=article_data, headers=headers)
        if response.status_code == 201:
            response_data = response.json()
            logging.info(f"ğŸ“¥ FastAPI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {response_data}")

            # âœ… ìˆí¼ ìƒì„± ì™„ë£Œ or ì‹¤íŒ¨ ë¡œê·¸ ë‚¨ê¹€
            if response_data.get("status") == "success":
                logging.info(f"âœ… ìˆí¼ ìƒì„± ì™„ë£Œ: {article_data['title']}")
            else:
                logging.warning(f"âš ï¸ ìˆí¼ ìƒì„± ì‹¤íŒ¨: {article_data['title']}, ì´ìœ : {response_data.get('message', 'ì•Œ ìˆ˜ ì—†ìŒ')}")

        else:
            logging.error(f"âŒ BiteNews ì˜¤ë¥˜ {response.status_code}: {response.text}")
    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ BiteNews ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")

# â³ 3ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ í•„í„°ë§ í•¨ìˆ˜
def filter_recent_articles(article_urls):
    valid_articles = []
    three_hours_ago = CURRENT_TIME - timedelta(hours=3)

    for url in article_urls:
        article_data = scrape_article(url)
        
        # 3ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ ì¶”ê°€
        if article_data["timestamp"] >= three_hours_ago:
            article_data["timestamp"] = article_data["timestamp"].strftime("%Y-%m-%dT%H:%M:%S+09:00")
            valid_articles.append(article_data)

    return valid_articles

# ğŸ“Œ í—¤ë“œë¼ì¸ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def check_for_new_articles(base_url, category):
    try:
        news_data = []
        response = requests.get(base_url, headers=header)
        soup = BeautifulSoup(response.text, "html.parser")
        headline_list = soup.find("ul", class_="sa_list")
        
        if not headline_list:
            logging.warning(f"{category}: í—¤ë“œë¼ì¸ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return

        for item in headline_list.find_all("li", class_="sa_item _SECTION_HEADLINE")[:5]:
            title_tag = item.find("a", class_="sa_text_title")
            title = title_tag.find("strong", class_="sa_text_strong").text.strip()
            link = title_tag["href"]
            news_data.append({'title': title, 'link': link})

        # ğŸ”¥ 3ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ í•„í„°ë§
        article_links = [news["link"] for news in news_data]
        recent_articles = filter_recent_articles(article_links)

        # ê²°ê³¼ ì¶œë ¥
        for article in recent_articles:
            post_article_to_server(article)

    except Exception as e:
        logging.error(f"{category} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

# ğŸ”„ ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜
def scrap_all_categories():
    for category, base_url in categories.items():
        logging.info(f"ğŸ“¡ {category} í¬ë¡¤ë§ ì‹œì‘ ({base_url})")
        check_for_new_articles(base_url, category)
        time.sleep(1)  # ìš”ì²­ ê°„ê²© ì¡°ì •

# AWS Lambdaìš© í•¸ë“¤ëŸ¬ í•¨ìˆ˜
def lambda_handler(event, context):
    logging.info("Lambda ì‹¤í–‰ ì‹œì‘")
    scrap_all_categories()
    return {
        "statusCode": 200,
        "body": json.dumps({"message": "Lambda ì™„ë£Œ"})
    }
